# -*- coding: utf-8 -*-
"""Prediccion_Proyecto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zc-t6PB_NAkcoBFO9-jf9tJtkVQLUIUy

# Importamos las librerias
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import sklearn
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_log_error

"""# Importamos la dataset"""

from google.colab import files
car = files.upload()

df = pd.read_csv("car data.csv")
print(df.shape)
df.head()

"""Este es un conjunto de datos de vehículos de cardekho Dataset. Este conjunto de datos contiene información sobre los automóviles usados ​​que figuran en el sitio web cardekho.com. Lo utilizaremos para encontrar predicciones de precios con el uso de modelos de regresión.

Los conjuntos de datos constan de varias variables independientes que incluyen:

Car_Name: esta columna contiene el nombre del automóvil.

Year: Esta columna se llena con el año en que se compró el auto.

Selling_Price: esta columna es el precio al que el propietario desea vender el automóvil.

Present_Price: este es el precio actual en la sala de exposición del automóvil.

Kms_Driven: Es la distancia recorrida por el coche en km.

Fuel_Type: tipo de combustible del automóvil, es decir, diésel, gasolina, GNC.

Seller_Type: Define si el vendedor es un distribuidor o un particular.

Transmission: Define si el auto es manual o automático.

Owner: Define el número de propietarios que ha tenido anteriormente el coche.
"""

print(df['Seller_Type'].unique())
print(df['Fuel_Type'].unique())
print(df['Transmission'].unique())
print(df['Owner'].unique())

#comprobar valores faltantes
df.isnull().sum()

df.describe()

"""En este modelo de regresión, la variable dependiente será 'Selling price', el resto, todas las variables se considerarán variables independientes.

Para construir un modelo de regresión lineal y polinomial necesitamos todas las variables numéricas, por lo que las características que contienen el tipo de datos de objeto se convierten o se eliminan.

#### Eliminando la columna 'Car_Name' ya que no contribuye a la toma de decisiones
"""

finalD = df.drop(['Car_Name'], axis = 1)

"""#### Reemplazo de la columna 'Year' por 'Age'"""

finalD['Age'] = 2022-finalD['Year']

finalD.drop(['Year'], axis = 1, inplace = True)

finalD.head()

"""# Funciones numéricas"""

fig, ax = plt.subplots(ncols=3, figsize=(20, 3))

sns.regplot(x=finalD.Age, y=finalD.Selling_Price, ax = ax[0])
sns.regplot(x=finalD.Kms_Driven, y=finalD.Selling_Price, ax = ax[1])
sns.regplot(x=finalD.Present_Price, y=finalD.Selling_Price, ax = ax[2])
#Aqui veo la relacion entre la edad del carro, kilometro recorridos y precio actual con el precio de venta

numerical_data = finalD[['Age', 'Kms_Driven', 'Present_Price']]
plt.figure(figsize=(10, 5))
sns.heatmap(numerical_data.corr(), annot=True)

"""#### No se observó una correlación importante

# Variables categoricas
"""

sns.countplot(df['Fuel_Type'])
plt.show()

sns.countplot(df['Seller_Type'])
plt.show()

sns.countplot(df['Transmission'])
plt.show()

fig, ax = plt.subplots(ncols=2, nrows = 2, figsize=(15, 8))

sns.boxplot(y=finalD.Selling_Price, x=finalD.Owner, ax = ax[0,0])
sns.boxplot(y=finalD.Selling_Price, x=finalD.Fuel_Type, ax = ax[0,1])
sns.boxplot(y=finalD.Selling_Price, x=finalD.Transmission, ax = ax[1,0])
sns.boxplot(y=finalD.Selling_Price, x=finalD.Seller_Type, ax = ax[1,1])
#Aqui observamos las variables categoricas y como se relacionan con  el precio de venta

"""1. Los automóviles diésel tienen el precio de venta más alto y están presentes la mayor cantidad de valores atípicos.
 Diésel > GNC > Gasolina en términos de precio de venta
2. Los coches automáticos son más caros que los manuales
3. Los coches sin dueño anterior son más caros que con dueño anterior.
4. Las personas venden sus automóviles a un precio menor que los que venden los concesionarios.

#### Evidentemente, el precio de venta se ve afectado por variables categóricas

### One hot encoding data
Para cambiar las variables categoricas
"""

df.Fuel_Type.value_counts()

df.Seller_Type.value_counts()

df.Transmission.value_counts()

finalD = pd.get_dummies(finalD, drop_first = True)
finalD.head()

"""##Dividimos en entrenamiento y test"""

from sklearn.model_selection import train_test_split

# Dividir los datos en conjuntos de entrenamiento y prueba
train, test = train_test_split(finalD, test_size=0.2, random_state=42)
X_train = train.drop(['Selling_Price'], axis=1)
y_train = train['Selling_Price']
X_test = test.drop(['Selling_Price'], axis=1)
y_test = test['Selling_Price']
print(f"Entrenamiento: {X_train.shape}")
print(f"Prueba: {X_test.shape,y_test.shape}")

# Crear y entrenar el modelo de Regresión Lineal
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin = lin_reg.predict(X_test)

# Crear y entrenar el modelo de Regresión Polinomial
poly_features = PolynomialFeatures(degree=2)
X_train_poly = poly_features.fit_transform(X_train)#Se ajustan y transforman los datos de entrenamiento X_train para incluir características polinomiales
X_test_poly = poly_features.transform(X_test)

poly_reg = LinearRegression()
poly_reg.fit(X_train_poly, y_train)
y_pred_poly = poly_reg.predict(X_test_poly)

# Asegurar de que no hay predicciones negativas
y_pred_lin = np.maximum(0, y_pred_lin)
y_pred_poly = np.maximum(0, y_pred_poly)

# Calculamos R² y RMSLE para Regresión Lineal y  Regresión Polinomial
r2_lin = r2_score(y_test, y_pred_lin)
rmsle_lin = np.sqrt(mean_squared_log_error(y_test, y_pred_lin))

r2_poly = r2_score(y_test, y_pred_poly)
rmsle_poly = np.sqrt(mean_squared_log_error(y_test, y_pred_poly))


# Imprimir los resultados
print("Linear Regression:")
print(f"R² score: {r2_lin}")
print(f"RMSLE: {rmsle_lin}")

print("\nPolynomial Regression:")
print(f"R² score: {r2_poly}")
print(f"RMSLE: {rmsle_poly}")

# Graficar los resultados
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(y_test, y_pred_lin, edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred_poly, edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Polynomial Regression')

plt.show()

"""Un valor de R² cercano a 1 indica que el modelo explica bien la variabilidad de los datos, mientras que un valor cercano a 0 indica que el modelo no explica bien la variabilidad.

Un RMSLE más bajo indica un mejor rendimiento del modelo, con predicciones más cercanas a los valores reales.
"""

from sklearn.linear_model import Ridge

#Regularización Ridge
ridge_reg = Ridge(alpha=1.7)
ridge_reg.fit(X_train, y_train)
y_pred_ridge = ridge_reg.predict(X_test)

# Asegurar de que no hay predicciones negativas
y_pred_ridge = np.maximum(0, y_pred_ridge)

# Calculamos R² y RMSLE para Regresión Lineal con Regularización Ridge
r2_ridge = r2_score(y_test, y_pred_ridge)
rmsle_ridge = np.sqrt(mean_squared_log_error(y_test, y_pred_ridge))

# Imprimir los resultados
print("Ridge Regression:")
print(f"R² score: {r2_ridge}")
print(f"RMSLE: {rmsle_ridge}")

#Regularización Ridge
ridge_poly_reg = Ridge(alpha=1.7)
ridge_poly_reg.fit(X_train_poly, y_train)
y_pred_ridge_poly = ridge_poly_reg.predict(X_test_poly)

# Asegurar de que no hay predicciones negativas
y_pred_ridge_poly = np.maximum(0, y_pred_ridge_poly)

# Calculamos R² y RMSLE
r2_ridge_poly = r2_score(y_test, y_pred_ridge_poly)
rmsle_ridge_poly = np.sqrt(mean_squared_log_error(y_test, y_pred_ridge_poly))

print("Ridge Polynomial Regression:")
print(f"R² score: {r2_ridge_poly}")
print(f"RMSLE: {rmsle_ridge_poly}")

# Crear una tabla comparativa
comparison = pd.DataFrame({
    'Model': ['Linear Regression', 'Polynomial Regression'],
    'R²': [r2_lin, r2_poly],
    'RMSLE': [rmsle_lin, rmsle_poly]
})

print(comparison)

# Crear una tabla comparativa
comparison2 = pd.DataFrame({
    'Model': ['Linear Regression ridge', 'Polynomial Regression ridge'],
    'R²': [r2_ridge, r2_ridge_poly],
    'RMSLE': [rmsle_ridge, rmsle_ridge_poly]
})

print(comparison2)
